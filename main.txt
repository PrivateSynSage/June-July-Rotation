\documentclass{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[toc,page]{appendix}
\usepackage{subcaption}
\usepackage[]{graphicx}
\usepackage{natbib}
\usepackage[default]{gfsbodoni}
% \usepackage{newpxtext,newpxmath}
\usepackage{csvsimple}
\usepackage[]{fixmath}
%\usepackage[toc,page]{appendix}

% Meta information
\title{June/July Rotation}
\author{Ryan Young}
\date{July 2016}

% Document geometries
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\begin{document}

\maketitle
\clearpage

\tableofcontents\clearpage

\section{Abstract}

Itinerant, multi-stable networks constitute an interesting class of networks capable of doing interesting tasks like counting, measuring duration, or intensity, even all at once\cite{miller_stimulus_2013}. Here, we wanted to investigate whether same network could account for some of the interesting network behavior noticed in by Mante et al. \cite{mante_context-dependent_2013}. To do so, we set up a simple firing rate network, and provided inputs that mimic the task. We then attempt to measure and observe the response with the same targeted dimensionality reduction technique.

\section{Introduction}

In 2013, Mante and colleagues conducted a now widely-known, interesting experiment, to better understand prefrontal computation. They looked at state space signals from a network involved in making decisions during eye saccade. Using an array of electrodes and monkeys doing a simple context-dependent task, they recorded FEF cells, most of whom had mixed information about stimuli and choice. The task had two contexts; in one, monkeys were to pay attention to the motion of dots and not color, saccading toward the dot motion, after a delay period. In the second context, they were to pay attention to the color, not the motion, saccading according to a rule, ``left green, right red''\ref{fig:task}. The dynamics were then cast into a task-relevent, PCA-derived, state space. \ref{fig:attractor}.

In the state space, 100 ms after context and stimuli begin, prefrontal ensembles appear to represent all these task variables and do not seem to filter out---nor receive a filtered out copy of stimuli from posterior cortex---contextually irrelevant data. During this presentation the network concomitantly and increasingly represents impending choice. Part ways to fully representing the choice and while the dot stimuli are still on, stimulus representations abate to near their starting value.

\begin{figure}[h]
  \includegraphics[width=1\textwidth]{pic/task.png}
  \caption{Task, from Mante et al. 2013}
  \label{fig:task}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=1\textwidth]{pic/datastatespace.png}
  \includegraphics[width=1\textwidth]{pic/statespace2.png}
  \caption{State space, from Mante et al. 2013}
  \label{fig:attractor}
\end{figure}

Subsequently, they attempted to fit the data, using a network model, wired by FORCE learning, a trick developed in the last decade to get a randomly connected network to closely reproduce an output \cite{sussillo_generating_2009}. While the output fit well, as FORCE learning is designed to do, it is questionable whether the method actually wired a network implementation with biologic realism. The network wired under conditions that are highly unlikely be found in vivo; Unit-to-unit weights are rapidly tuned fast enough that error stay first order, at all time steps, and the network output remains close to desired output, sampling only tiny perturbations. Real networks do not rapidly quench error, and it would not be surprising if the output model did not tell you much about the brain’s implementation of decision-making\footnote{This is suggested by a result of Moore in 1956: He showed \cite{church_moore_1956} the number of alternative internal mechanisms inside a black box constructed of elements, like circuits/automata, are infinite}. Subsequently they used this network and perturbation theory to try to understand how it mechanistically explains the biological data moving through state space.

The network model built here attempts to mimic that data, instead, using an itinerant, multi-stable, inhibition-balanced network. Its three prominent characteristics endow properties seen in biological networks. It utilizes discrete low-energy attractor states--much like discretized Hopfield nets--to represent stimuli\cite{hopfield_neural_1982}. It, second, utilizes an inhibitory-balance regime: a connection paradigm with strong excitatory recurrence balanced by powerful inhibition (often from more diffuse sources). These, likewise, explain paradoxical qualities in natural networks \cite{tsodyks_paradoxical_1997} \cite{csicsvari_reliability_1998} \cite{ozeki_inhibitory_2009}. Third, the network relies on synaptic depression or noise to kick it out of an existing state after some time and shift to another, with some history dependence to previous stimuli. \footnote{strength of history, too, depends on noise, which every so often can fork off some deterministic cycle of states, which can be a mechanism that helps to reproduce.} Indeed, what we seek to have the task with this type of model, and then probe its dynamics to hopefully say more about a local circuit decision dynamic.

\section{Methods}

\subsection{Firing Rate Model}

To model the network, we use a continuous firing rate model. The variables track the average properties experienced by a group of neurons, as opposed to a spiking model, where each variable describes experiences of a cell\footnote{From this point, if ``neuron’’ or ``cell’’ is mentioned regarding this model’s elements, that should translate to ``unit’’.} The cells have three major time-varying properties governing $r_i(t)$, the input to the cell: the input to the cell (carrying task variables and calculated synaptic inputs) $I_i(t)$, synaptic inputs controlling interaction strength between cells $S_i(t)$, and finally a key depression variable per synapse $D_i(t)$ determining the drop in synaptic strength as a cell repeatedly fires. Their relationships come together in the following hierarchy of equations, starting with firing rate $$\tau_r \frac{dr_i}{dt} = -r_i(t) + \frac{r^{max}_i}{ exp \{ [ \Theta_i - I_i(t)  ] / \Delta_i  \}}$$ in which $\Theta_i$ is that cells firing threshold and $\Delta_i$ controls it’s responsiveness to current. 

Included in that current $I_i(t)$ are the synaptic currents $S_i(t)$, who follow $$ \tau_s \frac{ds_i}{dt} = -s_i(t) +  \tilde{\alpha} p_0 r_i(t) \tau_s D_i(t).$$ The $\tilde{\alpha}$ (fraction of available binding sites) and $p_0$ (the base release probability) together determine the synaptic activity increase due to action potentials $r_i(t)\tau_s$. When further scaled by $D_i(t) \in [0,1]$ it creates a drop in $\frac{ds_i}{dt}$ for persistant firing. This scaling factor drops with firing, $$\tau_{Di}\frac{dD_i}{dt} = 1 - D_i(t) - p_0 r_i(t) \tau_{Di} D_i(t).$$

Last but not least, the actual input itself. $I_j(t)$ tracks three major sources of current---synaptic, applied current, and noise---who each get a term in the equation, $$ I_j(t) = \sum_i s_i(t)W_{i\rightarrow j} + I_j^{app}(t) + \sigma\eta(t). $$

\subsection{Stimulus}

Three different stimuli were created, one for context $I_{c=1}^{app}(t) \in \{-1,1\}$, motion $I_{c=2}^{app}(t) \in \{-3,-2,...,3\}$, and color $I_{c=3}^{app}(t) \in \{-3,-2,...,3\}$. For each $I_{c}^{app}$, we select at random a fraction of neurons to receive that current. The amplitude strength and the sign opposite stimuli concepts. Two different regimes were tried to  treat ``opposites''. Onsets and offsets where modeled like in the Mante et al. work, except for the variable delay period, where here we make all trials 2 seconds long \ref{figure:input}A.

\begin{figure}[h]
  \includegraphics[width=1\textwidth]{pic/stimuli.png}
  \caption{Stimuli}
  \label{fig:input}
\end{figure}

In one regime, ``opposites''---dots left versus right, color red versus green---were handled with excitatory versus inhibitory current of that population. However, it was noticed because some stimuli combinations when randomly sampled were all strong negative currents, and the network could be kicked out of all attractor states or fail to shift states. I re-gauged the interaction in two ways: one by raising recurrence, lowering inhibition; the other by changing the representation of an opposite from inhibitory current into two non-overlapping populations who both receive excitatory stimulus, each  a small fraction, $15\%-25\%$ of units.

Because, each stimuli picked units at random, this creates a situation where there is a $p_{frac}^2$ chance of unit selective for both stimuli, and a $p_{frac}^3$ chance for triple mixed-selectivity. For decision networks, previous work recognizes mixed selectivity as an important feature for learning complex choices\cite{fusi_why_2016}\cite{rigotti_internal_2010}.

There may well be a better ways to embed the stimuli, but these are meant a first approximations attempt.

\subsection{Connections}

We assigned strong EI and IE connections, overall weights of $320$ and $-320$, respectively; each was scaled by the average number of neurons sending out a connection of that type, per neuron. So, if there is one I-neuron $N_i=1$, its weight to all E-neurons is $-320$. If however, $N_i=5$ then average weight cuts by $5$ to $-64$.

We then progressively raise EE connections until units could be seen entering discrete attractor states, indicated by hot streaks across their firing rate rasters (Fig.\ref{fig:overunder}). Either to one of two levels were chosen. \begin{enumerate} \item Discrete attractors only within the bounds of the trial, that reset after stimuli stop (Fig \ref{fig:overunder}, left). \item Discrete attractors who often continue or itenerate in time between trials (Fig \ref{fig:overunder}, right).\end{enumerate}

We used two types of EE connections, reccurent connections and asymmetric. Recurrent connections took the form $W_{EE}^{i\rightarrow i}$ for excitatory groups connecting onto themselves\footnote{A more nuanced model of recurrence not implemented would be establishing groups of units who are considered a ``self'' and each set strongly connects within its ranks}. These connections were strong enough (among connected neurons who share stimuli) to locally overcome I-supression. Asymmetrics are E-to-E as well, but weaker, who affect the form of attractors and encourage new attractor states and when recurrent synaptic activity depressed.

Figure \ref{fig:connections} summarizes the typical connections expressed in a network instance. Not all runs had these exact weights, but the numbers remained quite close, and were varied to generate networks that would fire through the session, on only in a trial, and as well as varied for the two types of input.

\begin{figure}[h]
    \includegraphics[width=1\textwidth]{pic/connections.png}
    \caption{Connections}
    \label{fig:connections}
\end{figure}


In general, the input type that used negative current to represent an opposite requires a higher current to acheive firing in stable attractors. 

\section{Results}

\subsection{Tuning}

To find proper network states, we looked for instantiations of the network that reacted to each stimulus, and who appeared to shift attractors within the trial. Patterns were sought where the predominant number of state shifts happened within a trial---those where a preponderance of states did not carry too long (see Fig. \ref{fig:badstates} left), nor seem too punctate (\ref{fig:badstates} left). The reasoning behind this: Mante et al.'s data suggested task population activity rose and dropped to zero within stimulus presentation\footnote{Their state space graph starts 100ms after dot onset and stops 100ms after offset.}

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{pic/long.png}
    \includegraphics[width=0.5\textwidth]{pic/short.png}
    \caption{Rejected Network Examples}
    \label{fig:badstates}
\end{figure}

After exhaustively exploring a large parameter space (around 8000 different combinations of $W_{x}$ parameters), I settled on two types of network activity vis-a-vis two input types, mentioned above. It was not totally clear to me whether it was more important to have states contained in the trial or have a couple neurons carrying information between trials. But the guess was that memory property in the activity between states was only going to increase confusion in the network and act like a source of noise, because each trials stimuli were independently sampled, de novo.

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{pic/al_undertrial.png}
    \includegraphics[width=0.5\textwidth]{pic/al_overtrial.png}
    \caption{Accepted Network Examples}
    \label{fig:overunder}
\end{figure}

\subsection{State Space: Targeted Dimensionality Reduction}

To see if responses in networks resembled Mante data, I needed to see the activity in the same state space. Running between $50-400$ trials and $100-400$ units, the same ``Targeted Dimensionality Reduction'' was employed. 

To do so, GLM was conducted, to predict population activity from task variables, $ F_i \beta_i = r_i$. This was done with $F_i$, a matrix describing stimuli presented each trial and solving the equation for each cell $i$ yielded coefficients $\beta_i$. From there, a denoising matrix $D$ was made using the 12 strongest PCA eignvectors. Whereon, we merge $\beta$s and apply the denoising matrix to a each condition-indexed page of an $N_{cell} \times N_{sample} \times N_{conditions}$ matrix. Last, QR orthogonalization recovers the axis components. The resulting vectors describe the directions in population activity that ``best''\footnote{at least in linear land} explain task aspects.

Second, population averages had to be created for each particular condition found and then plotted, and separatelty, for averages for each triplicate condition permutation. So, for instance, an average of dot strength=3 trials encompassed an average of all trials with dot strength of 3, regardless of any other variables. For the permutation conditions, trials were collected only when they matched on all three trial variables, and population averaged. 

These averages were projected onto the axes given above. State spaces varied \textit{consirably} on network instantiation, though for a certain parameter set, similar shapes often appeared. The state space also seemed loosely ranked at best across a condition axis for the different strengths. This ranking appeared to me to get slightly better with higher sample sizes, squinting, but I have yet to try anything about 500 trials. Spaces may have a clearer picture with higher sample size. There are 72 different stimulus combinations. So in order to sample all 72, 100 to 1000 times, it requires between 700-7000 trials. Above 800 trials, it crashed an 80GB RAM machine I attempted to SSH the job to. Clearly a higher sample size would be nice. But it might be irksome to breakup the model into smaller runnable chunks without a big data tool (hadoop) or space optomizing existing code. A simpler solution might just be to reduce the number of possible stimuli that can happen, in a session, and then combine results from sessions.

Of the 2x2 categories of network types (within/between attractors, population/inhibition opposites) tested, state spaces appeared much tighter for the opposite-based opposites, and tended to be more decodable (see below). They more often appeared as sharp traces, shooting directly to an attractor state, sticking for the stimulus, and then returning sharply. Population-based stimulus seemed to increase the complexity in state space, with more curved, broad-stroke paths (perhaps a sign of more local attractors along the network path). Allowing long attractors bleeding over trials introduced more loop and copmlexity, and even created some seemingly heteroclinic paths, that looked like squares \ref{}. However, these models with activity crossing trials experience a large increase in their inability to read out a would-be choice from the activity (see next section). This may be due to the linear methods used for the read-out and also due to the LDA lacking information during training about past trials, needed to make sense of firing that persists from past trials.

Unfortunately, none of the model curves exhibited a shape strongly resembling the bottom row of \ref{fig:statespace}. The closest seen is shown in Figure \ref{fig:closest}.

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{pic/al_undertrial.png}
    \includegraphics[width=0.5\textwidth]{pic/al_overtrial.png}
    \caption{Accepted Network Examples}
    \label{fig:overunder}
\end{figure}

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{pic/al_undertrial.png}
    \includegraphics[width=0.5\textwidth]{pic/al_overtrial.png}
    \caption{Closest Model}
    \small Also one of the runs with the highest sample size run
    \label{fig:overunder}
\end{figure}




\subsection{Linear Discriminant Prediction}

Because we had not figured out how to properly install a plastic choice network or learning rule, the question was simply asked, can a simple statistical classifier trained on the half the data set predict the would-be choice from the activity. Presumably, if a simple linear discrimant analysis can find predictive components, so too can a simple neural net reading out network activity. They have similar solution spaces\footnote{... energy function minmized to solve an LDA has a form similar to a perceptron.}.

As alluded above, best predicted trails came from networks that didn't fire strongly between trials. Population and inhibition opposites worked similarly well for that network. The vast majority of sessions had fairly high predictability, 70\%, with some suprises mixed in. Asymmetric connections were randomly selected, occassionally producing networks who failed to transmit activity about the stimuli or times randomly sampled by the LDA failed to capture a brief, key moment that a attractor stored stimulus identity.

Between trial networks did particularly bad, for a reason highlighted above: LDAs are trained with one trials worth of data per prediction, and perhaps lack the panopia of extra trials to predict clearly.

\section{Discussion}

\subsection{Choices?}

How to get the network to choose? One possible way is to create a separate network, and train/reward it for correct choice output based on this input network's input.

Another more natural, perhaps internal way: have neurons capable of making a choice embedded in the network, randomly receiving connections from inputs. When a correct choice is made, do the network equivalent of diffuse dopamine release, rewarding recent stimulus histories with some decaying curve\cite{gomperts_vta_2015}.

\subsection{What could the model need to better replicate state space?} 

One thing mention that could help: a bigger trial size. This helps the GLM form a more accurate representation of the neurons involved in task variables.

Second, state spaces trace out smoother arcs with, perhaps because of a greater number of attractors encountered when pushed towards the final attractor state. It might help to increase the number of heterostable states itenerated over within the stimulus window. Maybe that means increasing the synaptic depression and asymmetric weights.


\section{Conclusion}

How networks of the brain pull off a decision at in a circuit is a hard scientific problem. But it may be yet be solvable with more realistic models to help explain the complex dynamics seen. Here, we attempted more bioligcal plausible model of neural data and while the current implementation does not yet replicate Mante's data, some changes proposed may help in the future.


\bibliographystyle{unsrt}
\bibliography{Miller}

\begin{appendices}

\section{More Exhaustive (Typical) Properties}

\begin{tabular}{l|c}%
\bfseries Property & \bfseries Value% specify table head
\csvreader[head to column names]{csv/consts.csv}{}% use head of csv as column names
{\\\hline \constants & \value}%
\end{tabular}

\section{Typical Input Pattern Within a Session}

Figure summarizing on the right input sources within a set of trials and on the left, input averaged across neurons.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{pic/InputSep.png}
    
\end{figure}


\end{appendices}

\end{document}
